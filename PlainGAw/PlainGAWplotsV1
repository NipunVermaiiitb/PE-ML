import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay, accuracy_score,
    precision_score, recall_score, f1_score, roc_auc_score
)
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from deap import base, creator, tools, algorithms
import random
import matplotlib.pyplot as plt
import xgboost as xgb
import warnings
warnings.filterwarnings("ignore")

# ---------------------------------------------------------
# Load dataset
# ---------------------------------------------------------
df = pd.read_csv('/content/All_subjects_segments60.csv')

drop_cols = ['Subject_ID','ApEn','scope','segment','SubjectID','SubjectName']
df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')

df['label'] = df['condition'].map({
    'Baseline': 0, 'Recovery': 0,
    'Stroop': 1, 'MAT': 1
})
df = df.drop(columns=['condition'], errors='ignore')

X = df.drop(columns=['label'])
y = df['label'].astype(int)

# ---------------------------------------------------------
# Train-test split FIRST
# ---------------------------------------------------------
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# ---------------------------------------------------------
# Impute + Scale Train Only
# ---------------------------------------------------------
imputer = SimpleImputer(strategy='median')
X_train_imp = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=X.columns)
X_test_imp = pd.DataFrame(imputer.transform(X_test_raw), columns=X.columns)

scaler = RobustScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imp), columns=X.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test_imp), columns=X.columns)

# ---------------------------------------------------------
# GA Feature Selection on Train Only
# ---------------------------------------------------------
X_full = X_train_scaled.copy()
n_features = X_full.shape[1]

# Reset DEAP
if "FitnessMax" in creator.__dict__:
    del creator.FitnessMax
if "Individual" in creator.__dict__:
    del creator.Individual

creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_bool", random.randint, 0, 1)
toolbox.register("individual", tools.initRepeat, creator.Individual,
                 toolbox.attr_bool, n=n_features)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# GA tracking arrays
best_train_fitness = []
best_test_accuracy = []
feature_frequency = np.zeros(n_features)

# Fitness function
def eval_individual(individual):
    if sum(individual) == 0:
        return 0.0,

    cols = [X_full.columns[i] for i in range(n_features) if individual[i] == 1]
    X_sub = X_full[cols].values
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = []

    for tr, va in cv.split(X_sub, y_train):
        rf = RandomForestClassifier(n_estimators=150, random_state=42)
        rf.fit(X_sub[tr], y_train.iloc[tr])
        pred = rf.predict(X_sub[va])
        scores.append(accuracy_score(y_train.iloc[va], pred))

    return np.mean(scores),

toolbox.register("evaluate", eval_individual)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.15)
toolbox.register("select", tools.selTournament, tournsize=3)

# GA loop
pop = toolbox.population(n=60)
hof_main = tools.HallOfFame(1)
NGEN = 10

for gen in range(NGEN):
    offspring = algorithms.varAnd(pop, toolbox, cxpb=0.6, mutpb=0.3)

    fits = list(map(toolbox.evaluate, offspring))
    for fit, ind in zip(fits, offspring):
        ind.fitness.values = fit

    pop = toolbox.select(offspring, len(pop))
    hof_main.update(pop)

    # Track best train CV fitness
    best_fit = hof_main[0].fitness.values[0]
    best_train_fitness.append(best_fit)

    # Test accuracy for best individual each gen
    best_ind = hof_main[0]
    cols = [X_full.columns[i] for i in range(n_features) if best_ind[i] == 1]
    if len(cols) == 0:   # fallback
        cols = X_full.columns.tolist()

    Xtr_sel = X_train_scaled[cols].values
    Xte_sel = X_test_scaled[cols].values

    rf = RandomForestClassifier(n_estimators=150, random_state=42)
    rf.fit(Xtr_sel, y_train)
    y_pred_test = rf.predict(Xte_sel)
    best_test_accuracy.append(accuracy_score(y_test, y_pred_test))

    # Update feature frequency
    feature_frequency += np.array(best_ind)

    print(f"Gen {gen+1}/{NGEN} | Best Fitness={best_fit:.4f} | Test Acc={best_test_accuracy[-1]:.4f}")

# ---------------------------------------------------------
# Final selected features
# ---------------------------------------------------------
best_ind = hof_main[0]
selected_features = [X_full.columns[i] for i in range(n_features) if best_ind[i] == 1]
if len(selected_features) == 0:
    selected_features = X_full.columns.tolist()

X_train_sel = X_train_scaled[selected_features].values
X_test_sel = X_test_scaled[selected_features].values

# ---------------------------------------------------------
# CLASSIFIERS
# ---------------------------------------------------------
classifiers = {
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=42),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(kernel='rbf', probability=True, random_state=42),
    "Logistic": LogisticRegression(max_iter=600, random_state=42),
    "XGBoost": xgb.XGBClassifier(
        n_estimators=200, max_depth=3, learning_rate=0.1,
        subsample=0.8, colsample_bytree=0.8,
        eval_metric='logloss', random_state=42
    ),
    "MLP": MLPClassifier(hidden_layer_sizes=(20,), max_iter=600, random_state=42)
}

# ---------------------------------------------------------
# Evaluate Test Set
# ---------------------------------------------------------
results = []

for name, clf in classifiers.items():
    print(f"\n===================== {name} =====================")

    clf.fit(X_train_sel, y_train)
    y_pred = clf.predict(X_test_sel)
    y_prob = clf.predict_proba(X_test_sel)[:, 1] if hasattr(clf, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else float("nan")

    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(f"F1 Score : {f1:.4f}")
    print(f"AUC      : {auc:.4f}")

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues')
    plt.title(f"{name} - Test Confusion Matrix")
    plt.show()

    results.append({
        "Model": name,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1 Score": f1,
        "AUC": auc
    })

df_res = pd.DataFrame(results)
print("\n=================== FINAL TEST RESULTS ===================")
print(df_res)

# ---------------------------------------------------------
# PLOTS
# ---------------------------------------------------------

# 1. GA Convergence (Train CV fitness)
plt.figure(figsize=(7,5))
plt.plot(best_train_fitness, marker='o')
plt.title("GA Convergence Curve (Train CV Fitness)")
plt.xlabel("Generation")
plt.ylabel("Best Fitness")
plt.grid(True)
plt.show()

# 2. Train vs Test Error Curve
plt.figure(figsize=(7,5))
plt.plot(best_train_fitness, marker='o', label="Train CV Accuracy")
plt.plot(best_test_accuracy, marker='s', label="Test Accuracy")
plt.title("Train vs Test Accuracy per GA Generation")
plt.xlabel("Generation")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# 3. Feature Frequency Heatmap
plt.figure(figsize=(10,6))
plt.bar(X_full.columns, feature_frequency)
plt.xticks(rotation=90)
plt.title("GA Feature Selection Frequency Across Generations")
plt.ylabel("Selection Count")
plt.tight_layout()
plt.show()

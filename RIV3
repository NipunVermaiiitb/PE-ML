# ======================================================
# ðŸ§ª Outlier-Resilient RZ + LOSO CV + PCA/t-SNE + 3D plot
# ======================================================

!pip install -q scikit-learn pandas numpy matplotlib seaborn xgboost

import os, json, zipfile, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, silhouette_score
from sklearn.model_selection import train_test_split
from scipy.stats import pearsonr

warnings.filterwarnings("ignore")

# ====== CONFIG ======
csv_path = "/content/All_subjects_segments60.csv"   # <-- change if needed
outdir = "/content/refined_rz_cv_embeddings_outputs"
os.makedirs(outdir, exist_ok=True)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# ======================================================
# 1) Load & robust preprocessing
# ======================================================
df = pd.read_csv(csv_path)
print("âœ… Loaded:", df.shape)

# drop known-useless if present
for c in ["scope","segment","Subject_Name","ApEn"]:
    if c in df.columns:
        df.drop(columns=c, inplace=True)

# normalize condition labels
def norm_cond(x):
    lx=str(x).strip().lower()
    if lx in {"baseline","base"}: return "Baseline"
    if lx in {"stroop","mat","stress"}: return "Stress"
    if lx in {"recovery","post","post_stress"}: return "Recovery"
    return x
df["condition"]=df["condition"].apply(norm_cond)

# numeric columns (exclude Subject_ID from transforms)
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if c not in ["Subject_ID"]]

# winsorize 1/99
q_low, q_hi = df[num_cols].quantile(0.01), df[num_cols].quantile(0.99)
for c in num_cols:
    df[c] = np.clip(df[c], q_low[c], q_hi[c])

# log1p skewed positives
sk = df[num_cols].skew(numeric_only=True)
for c in num_cols:
    if (df[c] > 0).all() and abs(sk[c]) > 1.0:
        df[c] = np.log1p(df[c])

# impute + robust scale
imp = IterativeImputer(random_state=RANDOM_SEED, max_iter=15)
scaler = RobustScaler()
df[num_cols] = imp.fit_transform(df[num_cols])
df[num_cols] = scaler.fit_transform(df[num_cols])

# ======================================================
# 2) Subject-specific baseline centering
#    subtract each subject's baseline mean from all their rows
# ======================================================
baseline_means = (
    df[df["condition"]=="Baseline"]
    .groupby("Subject_ID")[num_cols]
    .mean()
    .add_prefix("BL_")
    .reset_index()
)
df = df.merge(baseline_means, on="Subject_ID", how="left")
for c in num_cols:
    blc = f"BL_{c}"
    if blc in df.columns:
        df[c] = df[c] - df[blc]
df.drop(columns=[c for c in df.columns if c.startswith("BL_")], inplace=True)

# ======================================================
# 3) Aggregate per Subject x Condition
# ======================================================
agg = df.groupby(["Subject_ID","condition"], as_index=False)[num_cols].mean()
wide = agg.pivot_table(index="Subject_ID", columns="condition", values=num_cols)
wide.columns = [f"{feat}__{cond}" for feat,cond in wide.columns.to_flat_index()]
wide = wide.reset_index()

def have_all(f):
    return all(f"{f}__{p}" in wide.columns for p in ["Baseline","Stress","Recovery"])
base_feats = [f for f in {c.split("__")[0] for c in wide.columns} if have_all(f)]
subjects = wide["Subject_ID"].values
print("Features with all phases:", len(base_feats))

# ======================================================
# 4) Compute SI, RI and Outlier-Resilient RZ
# ======================================================
eps = 1e-9

# per-feature measures
for f in base_feats:
    B, S, R = wide[f"{f}__Baseline"], wide[f"{f}__Stress"], wide[f"{f}__Recovery"]
    # Stress Index (log-based)
    wide[f"SI_{f}"] = np.abs(np.log1p(np.abs(S)) - np.log1p(np.abs(B)))
    # Recovery Index (fraction of gap closed)
    wide[f"RI_{f}"] = ((R - S) / (B - S + eps)).clip(0, 1)

# subject-level single indices
wide["SI_true"] = wide[[f"SI_{f}" for f in base_feats]].mean(axis=1)
wide["RI_true"] = wide[[f"RI_{f}" for f in base_feats]].mean(axis=1)

# Outlier-resilient resilience indices
RZ_ratio = wide["RI_true"] / (wide["SI_true"] + eps)
# primary: tanh-squashed ratio
wide["RZ_tanh_true"] = np.tanh(RZ_ratio)
# secondary: z-normalized ratio (also saved for completeness)
z_SI = (wide["SI_true"] - wide["SI_true"].mean()) / (wide["SI_true"].std() + eps)
z_RI = (wide["RI_true"] - wide["RI_true"].mean()) / (wide["RI_true"].std() + eps)
wide["RZ_z_true"] = z_RI / (z_SI + eps)

# ======================================================
# 5) Build modeling features (levels + deltas + protected ratios)
# ======================================================
X_parts = {}
for f in base_feats:
    B, S, R = wide[f"{f}__Baseline"], wide[f"{f}__Stress"], wide[f"{f}__Recovery"]
    X_parts[f"{f}__Baseline"] = B
    X_parts[f"{f}__Stress"]   = S
    X_parts[f"{f}__Recovery"] = R
    X_parts[f"{f}__SminusB"]  = S - B
    X_parts[f"{f}__RminusS"]  = R - S
    X_parts[f"{f}__RminusB"]  = R - B
    denomB = B.replace(0, np.nan)
    denomS = S.replace(0, np.nan)
    X_parts[f"{f}__S_over_B"] = (S / denomB).replace([np.inf,-np.inf], np.nan)
    X_parts[f"{f}__R_over_S"] = (R / denomS).replace([np.inf,-np.inf], np.nan)

X_all = pd.DataFrame(X_parts).fillna(0.0)

# Targets
targets = {
    "SI": wide["SI_true"].values,
    "RI": wide["RI_true"].values,
    "RZ": wide["RZ_tanh_true"].values,  # use tanh version as primary
}

# ======================================================
# 6) LOSO CV helpers (per subject) + feature selection
# ======================================================
def metrics(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    pr, pp = pearsonr(y_true, y_pred) if len(y_true) > 1 else (np.nan, np.nan)
    return dict(R2=r2, MAE=mae, RMSE=rmse, Pearson_r=pr, p_value=pp)

def fit_with_selection(model_name, Xtr, ytr, top_k=25, seed=42):
    if model_name == "RF":
        base = RandomForestRegressor(
            n_estimators=500, random_state=seed, n_jobs=-1, max_depth=None
        )
    elif model_name == "XGB":
        base = XGBRegressor(
            n_estimators=600, learning_rate=0.05, max_depth=4,
            subsample=0.9, colsample_bytree=0.9, random_state=seed, n_jobs=-1,
            reg_lambda=1.0, objective="reg:squarederror"
        )
    else:
        raise ValueError("Unknown model")
    base.fit(Xtr, ytr)
    importances = pd.Series(base.feature_importances_, index=Xtr.columns).sort_values(ascending=False)
    keep = importances.head(min(top_k, max(1, (importances > 0).sum()))).index.tolist()
    # retrain final
    if model_name == "RF":
        final = RandomForestRegressor(n_estimators=800, random_state=seed, n_jobs=-1, max_depth=None)
    else:
        final = XGBRegressor(
            n_estimators=800, learning_rate=0.05, max_depth=4,
            subsample=0.9, colsample_bytree=0.9, random_state=seed, n_jobs=-1,
            reg_lambda=1.0, objective="reg:squarederror"
        )
    final.fit(Xtr[keep], ytr)
    return final, keep, importances

def loso_cv(X, y, subj_ids, model_name, seed=42, top_k=25):
    unique_subj = np.array(sorted(np.unique(subj_ids)))
    preds, trues, heldout = [], [], []
    # Per-fold selected-feature tracking (optional)
    selected_counts = {}
    for s in unique_subj:
        te_mask = (subj_ids == s)
        tr_mask = ~te_mask
        Xtr, Xte = X.iloc[tr_mask], X.iloc[te_mask]
        ytr, yte = y[tr_mask], y[te_mask]
        model, keep, imps = fit_with_selection(model_name, Xtr, ytr, top_k=top_k, seed=seed)
        yhat = model.predict(Xte[keep])
        preds.append(yhat[0] if yhat.ndim==1 else yhat.squeeze()[0])
        trues.append(yte[0])
        heldout.append(s)
        for k in keep:
            selected_counts[k] = selected_counts.get(k, 0) + 1
    preds = np.array(preds); trues = np.array(trues); heldout = np.array(heldout)
    return preds, trues, heldout, selected_counts

# Which model per target (based on your earlier results)
model_for = {"SI":"XGB", "RI":"RF", "RZ":"RF"}

loso_results = {}
selected_feature_freq = {}

for target in ["SI","RI","RZ"]:
    y = targets[target]
    model_name = model_for[target]
    preds, trues, heldout, sel_counts = loso_cv(X_all, y, subjects, model_name=model_name, seed=22, top_k=25)
    loso_results[target] = {
        "metrics": metrics(trues, preds),
        "n_subjects": int(len(heldout)),
        "model": model_name
    }
    # Save per-subject LOSO predictions
    pd.DataFrame({"Subject_ID":heldout, f"{target}_true":trues, f"{target}_pred":preds})\
      .sort_values("Subject_ID").to_csv(f"{outdir}/loso_{target}_per_subject.csv", index=False)
    # Track selected features frequency
    selected_feature_freq[target] = dict(sorted(sel_counts.items(), key=lambda kv: kv[1], reverse=True))

# Save LOSO summary
with open(f"{outdir}/loso_summary.json","w") as f:
    json.dump(loso_results, f, indent=2)
with open(f"{outdir}/loso_selected_features.json","w") as f:
    json.dump(selected_feature_freq, f, indent=2)

# ======================================================
# 7) Embeddings & Clustering in (SI, RI, RZ_tanh) space
# ======================================================
emb_df = wide[["Subject_ID","SI_true","RI_true","RZ_tanh_true"]].copy()

# KMeans clustering (k=2) for resilient vs non-resilient (heuristic)
X_emb = emb_df[["SI_true","RI_true","RZ_tanh_true"]].values
scaler_emb = StandardScaler()
X_scaled = scaler_emb.fit_transform(X_emb)
kmeans = KMeans(n_clusters=2, random_state=RANDOM_SEED)
labels = kmeans.fit_predict(X_scaled)
emb_df["Cluster"] = labels

# PCA 2D
pca = PCA(n_components=2, random_state=RANDOM_SEED)
p2 = pca.fit_transform(X_scaled)
plt.figure(figsize=(6.2,5))
sns.scatterplot(x=p2[:,0], y=p2[:,1], hue=emb_df["Cluster"], s=80, palette="Set2")
plt.title("PCA of (SI, RI, RZ_tanh) space")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.tight_layout(); plt.savefig(f"{outdir}/pca_si_ri_rz.png", dpi=160); plt.show()

# t-SNE 2D (perplexity must be < n_subjects)
perp = max(5, min(30, len(emb_df)-5))
tsne = TSNE(n_components=2, perplexity=perp, learning_rate='auto', init='random', random_state=RANDOM_SEED)
t2 = tsne.fit_transform(X_scaled)
plt.figure(figsize=(6.2,5))
sns.scatterplot(x=t2[:,0], y=t2[:,1], hue=emb_df["Cluster"], s=80, palette="Set2")
plt.title(f"t-SNE of (SI, RI, RZ_tanh) space (perplexity={perp})")
plt.tight_layout(); plt.savefig(f"{outdir}/tsne_si_ri_rz.png", dpi=160); plt.show()

# 3D Plot
fig = plt.figure(figsize=(7,5.6))
ax = fig.add_subplot(111, projection='3d')
cmap = plt.get_cmap("Set2")
colors = [cmap(l) for l in emb_df["Cluster"]]
ax.scatter(emb_df["SI_true"], emb_df["RI_true"], emb_df["RZ_tanh_true"], s=60, c=colors, depthshade=True)
ax.set_xlabel("SI (log-based)"); ax.set_ylabel("RI"); ax.set_zlabel("RZ_tanh")
ax.set_title("3D SIâ€“RIâ€“RZ_tanh Space")
plt.tight_layout(); plt.savefig(f"{outdir}/3d_si_ri_rz.png", dpi=160); plt.show()

# Save embeddings & cluster table
emb_df.to_csv(f"{outdir}/embeddings_si_ri_rz_clusters.csv", index=False)

# Also compute silhouette on the 3D features
sil = silhouette_score(X_scaled, labels)
with open(f"{outdir}/cluster_quality.json","w") as f:
    json.dump({"silhouette_score": float(sil)}, f, indent=2)

# ======================================================
# 8) Final summary + ZIP
# ======================================================
summary = {
    "loso_results": loso_results,
    "cluster_silhouette": float(sil),
    "n_subjects": int(wide["Subject_ID"].nunique()),
    "notes": {
        "RZ_primary": "RZ_tanh = tanh(RI / (SI + eps))",
        "RZ_secondary": "RZ_z = z(RI) / (z(SI) + eps) saved in embeddings CSV",
        "models_per_target_LOSO": {"SI":"XGB", "RI":"RF", "RZ":"RF"}
    }
}
with open(f"{outdir}/summary.json","w") as f:
    json.dump(summary, f, indent=2)
with open(f"{outdir}/upload_me_for_analysis.json","w") as f:
    json.dump(summary, f, indent=2)

zip_path = "/content/refined_rz_cv_embeddings_outputs.zip"
with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
    for root, _, files in os.walk(outdir):
        for fn in files:
            p = os.path.join(root, fn)
            z.write(p, os.path.relpath(p, outdir))

print("\nâœ… Finished. All results saved.")
print("ZIP:", zip_path)
